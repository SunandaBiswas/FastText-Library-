{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FastText for Semantic Similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4w6qrc8x1bh"
      },
      "source": [
        "#**FastText for Semantic Similarity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAZSG73LvTGK"
      },
      "source": [
        " **FastText** is an extremely useful module for word embedding and text classification problems. **FastText** has been developed by Facebook and has shown excellent results on many NLP problems, such as **semantic similarity detection and text classification.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54sJInUPxqgV"
      },
      "source": [
        "This project is on how FastText library creates vector representations that can be used to find semantic similarities between the words. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGdSAWl-yYfo"
      },
      "source": [
        "Need to install Wikipedia Library for Python"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm9I9ckDvLzl",
        "outputId": "2314323e-2223-43c5-e8a2-44b425cbb0a8"
      },
      "source": [
        "pip install wikipedia "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11696 sha256=ff7f7b872419eca1fea0bb42a82c957fca1ab020590ba1957268df39a7d33260\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ump7BKsEzjyz"
      },
      "source": [
        "FastText supports both Continuous **Bag of Words** and **Skip-Gram models**.  Skip-gram model has been implemented to learn vector representation of words from the Wikipedia articles on different topics of artificial intelligence, machine learning, deep learning, and neural networks. I have chosen these to make a corpus with these similar topics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jH-bQkwFsSYy"
      },
      "source": [
        "## Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wr0RsQZAr4jx",
        "outputId": "7f293514-046f-4a13-cee5-bae4b0561374"
      },
      "source": [
        "import wikipedia\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4qaCMrErobd"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from gensim.models.fasttext import FastText \n",
        "#using the FastText module from the gensim.models.fasttext library. \n",
        "#For the word representation and semantic similarity, the Gensim model can be used for FastText.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKcYz7SzsaQp"
      },
      "source": [
        "##Scraping Articles from Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp03CzzttrJE"
      },
      "source": [
        "- To scrape a Wikipedia page,the page method has been used from the wikipedia module. The name of the page that is for scrapping purpose is passed as a parameter to the page method. \n",
        "- The method returns WikipediaPage object, that is used to retrieve the page contents via the content attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L55RHCCAsQFe"
      },
      "source": [
        "artificial_intelligence = wikipedia.page(\"Artificial Intelligence\").content\n",
        "deep_learning = wikipedia.page(\"Deep Learning\").content\n",
        "neural_network = wikipedia.page(\"Neural Network\").content\n",
        "\n",
        "### tokenized scraped text data into sentences using the sent_tokenize method.\n",
        "artificial_intelligence = sent_tokenize(artificial_intelligence)\n",
        "deep_learning = sent_tokenize(deep_learning)\n",
        "neural_network = sent_tokenize(neural_network)\n",
        "\n",
        "### sentences from the three articles are being joined here together via the extend method.\n",
        "artificial_intelligence.extend(deep_learning)\n",
        "artificial_intelligence.extend(neural_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnhGuEB1xcnR"
      },
      "source": [
        "#Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_O6UxuNyNXC"
      },
      "source": [
        "- Cleaning our text data by removing punctuations and numbers. \n",
        "- Converting the data into the lower case. \n",
        "- lemmatizing the words to their root form. \n",
        "- The stop words and the words with the length less than 4 will be removed from the corpus, that is chosen randomly for this test, so you may allow the words with smaller or greater lengths in the corpus.\n",
        "\n",
        "- The preprocess_text function, performs the preprocessing tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXe6qwKhuWj1"
      },
      "source": [
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(document):\n",
        "        # Removing all the special characters\n",
        "        document = re.sub(r'\\W', ' ', str(document))\n",
        "\n",
        "        # Removing all single characters\n",
        "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Removing single characters from the start\n",
        "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
        "\n",
        "        # Substituting multiple spaces with single space\n",
        "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "\n",
        "        # Removing prefixed 'b'\n",
        "        document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "        # Converting to Lowercase\n",
        "        document = document.lower()\n",
        "\n",
        "        # Lemmatization\n",
        "        tokens = document.split()\n",
        "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
        "        tokens = [word for word in tokens if word not in en_stop]\n",
        "        tokens = [word for word in tokens if len(word) > 3]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u20P4Hlny140",
        "outputId": "9ddee524-cc7c-48e3-d862-6f1d4a3eec00"
      },
      "source": [
        "###  CHECKING if the function performs the desired task by preprocessing a dummy sentence\n",
        "\n",
        "sent = preprocess_text(\"Hello! I'm here to check if my function performs the desired task by preprocessing a dummy sentence.... \")\n",
        "print(sent)\n",
        "\n",
        "\n",
        "final_corpus = [preprocess_text(sentence) for sentence in artificial_intelligence if sentence.strip() !='']\n",
        "\n",
        "word_punctuation_tokenizer = nltk.WordPunctTokenizer()\n",
        "word_tokenized_corpus = [word_punctuation_tokenizer.tokenize(sent) for sent in final_corpus]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hello check function performs desired task preprocessing dummy sentence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V-DDoaP2mj3"
      },
      "source": [
        "\n",
        "here, the punctuations and stop words have been removed, and the sentences have been lemmatized. Furthermore, words with length less than 4, such as \"era\", have also been removed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SegIcCIE3nQ7"
      },
      "source": [
        "## Creating Words Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQiqkFam3puZ"
      },
      "source": [
        "Preprocessing is done for our corpus. Next step is to create word representations using FastText. \n",
        "steps :\n",
        "- Firstly, defining the the **hyper-parameters** for our **FastText model**\n",
        "- **embedding_size, window_size, min_word, down_sampling** these are the important hyperparameters for fastText model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmfr9wbw13pk"
      },
      "source": [
        "embedding_size = 60\n",
        "window_size = 40\n",
        "min_word = 5\n",
        "down_sampling = 1e-2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmopUbSK4r0Z"
      },
      "source": [
        "- **embedding_size** :  the size of the embedding vector. In other words, each word in our corpus will be represented as a 60-dimensional vector.\n",
        "\n",
        "- **window_size** :  the size of the number of words occurring before and after the word based on which the word representations will be learned for the word.\n",
        "In the skip-gram model we input a word to the algorithm and the output is the context words. If the window size is 40, for each input there will be 80 outputs: 40 words that occur before the input word and 40 words that occur after the input word. The word embeddings for the input word are learned using these 80 output words.\n",
        "- **min_word** :  specifies the minimum frequency of a word in the corpus for which the word representations will be generated. \n",
        "- **down_sampling** : the most frequently occurring word will be down-sampled by a number specified "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUBFqAOr7fEc"
      },
      "source": [
        "## Creating FastText model for Word Representations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbzO-PIY4q5K",
        "outputId": "83edca7d-1ee1-4efa-c0b5-c5045e541f39"
      },
      "source": [
        "%%time\n",
        "ft_model = FastText(word_tokenized_corpus,\n",
        "                      size=embedding_size,\n",
        "                      window=window_size,\n",
        "                      min_count=min_word,\n",
        "                      sample=down_sampling,\n",
        "                      sg=1,\n",
        "                      iter=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 26.8 s, sys: 266 ms, total: 27.1 s\n",
            "Wall time: 25.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu1dyw7H8AFt"
      },
      "source": [
        "- **sg** parameter : defines the type of model. **sg = 1** specifies that we want to create skip-gram model and zero specifies the bag of words model, which is the default value as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCYpJmD28r-m"
      },
      "source": [
        "### Checking the word representation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8BTzqC9NMw"
      },
      "source": [
        "using **wv** method to check the word representation for the words \"Sunanda\"\n",
        "- In the output it will show a 60-dimensional vector for the word \"Sunanda\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3XV9xEB7sER",
        "outputId": "a9e1d1db-0244-4b00-93d1-6754e327f022"
      },
      "source": [
        "print(ft_model.wv['Sunanda'])  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.14300203  0.42836142 -0.19133398 -0.62088096  0.02198491  0.04482359\n",
            "  0.17954879 -1.009901    0.11191189  0.17173022 -0.8434557  -0.12193018\n",
            " -0.8853135   0.5310766  -0.1306121  -0.06348509  0.2299048  -0.8656431\n",
            " -0.89303595  0.80831206 -0.11225256 -0.5862316  -0.00986825 -0.20372969\n",
            " -0.11302911  0.71904874  0.2676304  -0.9952991   0.8046882   1.6498783\n",
            " -0.52074516 -0.27212813 -0.4511383  -0.59613365 -0.48741227 -0.6358657\n",
            " -0.37141305 -0.28175837 -0.41582265  0.38244763 -0.78257376  0.31139666\n",
            " -0.29351392 -0.21120054 -0.28536057  0.36699802 -0.7435014  -0.62926894\n",
            " -0.63642716 -0.1170398   0.11857605 -0.1451082   0.0335955   0.18470442\n",
            " -0.18027362  0.6841604  -0.60830015 -0.01248473 -0.17693934 -0.07898621]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHvGk7cm-Aw5"
      },
      "source": [
        "- Finding top 5 most similar words for the words 'artificial', 'intelligence', 'machine', 'network', 'recurrent', 'deep'. \n",
        "- Any number of words can be chosen. The following script prints the specified words along with the 5 most similar words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFV7uIbj8vNu",
        "outputId": "654276df-c34d-449c-e3d0-22d303343604"
      },
      "source": [
        "semantically_similar_words = {words: [item[0] for item in ft_model.wv.most_similar([words], topn=5)]\n",
        "                  for words in ['artificial', 'intelligence', 'machine', 'network', 'recurrent', 'deep']}\n",
        "\n",
        "for k,v in semantically_similar_words.items():\n",
        "    print(k+\":\"+str(v))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "artificial:['intelligence', 'superintelligence', 'social', 'policy', 'moral']\n",
            "intelligence:['artificial', 'superintelligence', 'intelligent', 'creating', 'turing']\n",
            "machine:['described', 'argument', 'intelligent', 'ethical', 'study']\n",
            "network:['neural', 'specifically', 'convolutional', 'biological', 'recurrent']\n",
            "recurrent:['supervised', 'current', 'unsupervised', 'drug', 'depth']\n",
            "deep:['learning', 'scale', 'specifically', 'abstract', 'depth']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_h2upWG-sUx"
      },
      "source": [
        "\n",
        "\n",
        "*   cosine similarity between the vectors for any two words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy6pC9tp-DWV",
        "outputId": "037eacc4-286a-4ae3-e83d-eedb514b5200"
      },
      "source": [
        "print(ft_model.wv.similarity(w1='artificial', w2='intelligence'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7221809\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}